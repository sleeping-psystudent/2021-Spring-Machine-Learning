{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LBC_MLSpring2021_HW2_1-Copy1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, \n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "58d02431-b264-4eaa-82fe-f2e111ec35f5"
      },
      "source": [
        "!gdown --id '1b2OQjIplZkDrRj8wW5G04xxjk5fra3QC' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1b2OQjIplZkDrRj8wW5G04xxjk5fra3QC\n",
            "To: /content/data.zip\n",
            "372MB [00:02, 157MB/s]\n",
            "Archive:  data.zip\n",
            "   creating: timit_11/\n",
            "  inflating: timit_11/train_11.npy   \n",
            "  inflating: timit_11/test_11.npy    \n",
            "  inflating: timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "ddb81a7f-3d75-4e1f-8831-1023b2dac09a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYqi_lAuvC59",
        "outputId": "b7e1dce4-95c0-468e-8e12-b638454e69fa"
      },
      "source": [
        "VAL_RATIO = 0.01\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (1217632, 429)\n",
            "Size of validation set: (12300, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUCbQvqJurYc"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "24efe83f-bfef-4d31-b562-88419da35b12"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# model definition\n",
        "class Classifier(nn.Module):\n",
        "    # define model elements\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.drop = nn.Dropout(p=p)\n",
        "        # input to first hidden layer\n",
        "        self.layer1 = nn.Linear(429, 2048, bias=True)\n",
        "        nn.init.kaiming_uniform_(self.layer1.weight, \n",
        "                                 nonlinearity = 'relu')\n",
        "        self.bn1 = nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, \n",
        "                                  affine=True, track_running_stats=True)\n",
        "        # second hidden layer\n",
        "        self.layer2 = nn.Linear(2048, 1024, bias=True)\n",
        "        nn.init.kaiming_uniform_(self.layer2.weight, \n",
        "                                 nonlinearity = 'relu')\n",
        "        self.bn2 = nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, \n",
        "                                  affine=True, track_running_stats=True)\n",
        "        # third hidden layer and output\n",
        "        self.layer3 = nn.Linear(1024, 64, bias=True)\n",
        "        nn.init.xavier_uniform_(self.layer3.weight)\n",
        "        self.bn3 = nn.BatchNorm1d(64, eps=1e-05, momentum=0.1, \n",
        "                                  affine=True, track_running_stats=True)        \n",
        "        self.out = nn.Linear(64, 39, bias=True) \n",
        "               \n",
        "        self.act_fn = nn.Sigmoid()\n",
        "        \n",
        "    # forward propagate input\n",
        "    def forward(self, x):\n",
        "        # input to first hidden layer\n",
        "        x = torch.relu(self.bn1(self.layer1(x)))\n",
        "        x = self.drop(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        # second hidden layer\n",
        "        x = torch.relu(self.bn2(self.layer2(x)))\n",
        "        x = self.drop(x)        \n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        # third hidden layer and output        \n",
        "        x = torch.relu(self.bn3(self.layer3(x)))      \n",
        "        x = self.act_fn(x)     \n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTp3ZXg1yO9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383fec29-21e1-42b3-dac8-ee1b2879fa72"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 600               # number of training epoch\n",
        "learning_rate = 0.00013       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                             lr=learning_rate, \n",
        "                             betas=(0.9, 0.99),\n",
        "                             eps =1e-06,\n",
        "                             weight_decay=0.000035,\n",
        "                             amsgrad= True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMWsBs7zzNs",
        "outputId": "b4afb772-cd6c-4e9b-fec4-391c72e10315"
      },
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        " \n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/600] Train Acc: 0.419604 Loss: 2.199993 | Val Acc: 0.537398 loss: 1.631927\n",
            "saving model with acc 0.537\n",
            "[002/600] Train Acc: 0.555610 Loss: 1.574444 | Val Acc: 0.618211 loss: 1.347752\n",
            "saving model with acc 0.618\n",
            "[003/600] Train Acc: 0.600649 Loss: 1.381407 | Val Acc: 0.647642 loss: 1.206731\n",
            "saving model with acc 0.648\n",
            "[004/600] Train Acc: 0.626120 Loss: 1.271107 | Val Acc: 0.669431 loss: 1.120180\n",
            "saving model with acc 0.669\n",
            "[005/600] Train Acc: 0.642707 Loss: 1.198302 | Val Acc: 0.676992 loss: 1.076676\n",
            "saving model with acc 0.677\n",
            "[006/600] Train Acc: 0.654502 Loss: 1.147848 | Val Acc: 0.679512 loss: 1.047646\n",
            "saving model with acc 0.680\n",
            "[007/600] Train Acc: 0.662751 Loss: 1.110974 | Val Acc: 0.689268 loss: 1.003990\n",
            "saving model with acc 0.689\n",
            "[008/600] Train Acc: 0.669508 Loss: 1.082702 | Val Acc: 0.697480 loss: 0.981819\n",
            "saving model with acc 0.697\n",
            "[009/600] Train Acc: 0.674681 Loss: 1.060239 | Val Acc: 0.694797 loss: 0.963134\n",
            "[010/600] Train Acc: 0.678882 Loss: 1.041387 | Val Acc: 0.705041 loss: 0.948923\n",
            "saving model with acc 0.705\n",
            "[011/600] Train Acc: 0.682655 Loss: 1.025023 | Val Acc: 0.706504 loss: 0.932236\n",
            "saving model with acc 0.707\n",
            "[012/600] Train Acc: 0.685589 Loss: 1.012389 | Val Acc: 0.702033 loss: 0.926014\n",
            "[013/600] Train Acc: 0.688743 Loss: 0.999713 | Val Acc: 0.708862 loss: 0.920095\n",
            "saving model with acc 0.709\n",
            "[014/600] Train Acc: 0.691143 Loss: 0.989881 | Val Acc: 0.713008 loss: 0.910357\n",
            "saving model with acc 0.713\n",
            "[015/600] Train Acc: 0.692805 Loss: 0.980866 | Val Acc: 0.711057 loss: 0.904780\n",
            "[016/600] Train Acc: 0.694831 Loss: 0.972591 | Val Acc: 0.715935 loss: 0.892937\n",
            "saving model with acc 0.716\n",
            "[017/600] Train Acc: 0.696824 Loss: 0.964899 | Val Acc: 0.714390 loss: 0.897486\n",
            "[018/600] Train Acc: 0.698667 Loss: 0.957852 | Val Acc: 0.713740 loss: 0.896234\n",
            "[019/600] Train Acc: 0.699876 Loss: 0.951928 | Val Acc: 0.722276 loss: 0.875513\n",
            "saving model with acc 0.722\n",
            "[020/600] Train Acc: 0.701259 Loss: 0.946544 | Val Acc: 0.718780 loss: 0.879541\n",
            "[021/600] Train Acc: 0.702876 Loss: 0.940791 | Val Acc: 0.716992 loss: 0.881015\n",
            "[022/600] Train Acc: 0.703689 Loss: 0.936515 | Val Acc: 0.722683 loss: 0.863345\n",
            "saving model with acc 0.723\n",
            "[023/600] Train Acc: 0.705651 Loss: 0.930790 | Val Acc: 0.719837 loss: 0.863171\n",
            "[024/600] Train Acc: 0.705744 Loss: 0.926798 | Val Acc: 0.718049 loss: 0.872128\n",
            "[025/600] Train Acc: 0.707272 Loss: 0.922689 | Val Acc: 0.721220 loss: 0.862514\n",
            "[026/600] Train Acc: 0.708129 Loss: 0.918903 | Val Acc: 0.725041 loss: 0.855210\n",
            "saving model with acc 0.725\n",
            "[027/600] Train Acc: 0.709226 Loss: 0.914064 | Val Acc: 0.722195 loss: 0.865315\n",
            "[028/600] Train Acc: 0.710105 Loss: 0.912429 | Val Acc: 0.722033 loss: 0.859680\n",
            "[029/600] Train Acc: 0.710770 Loss: 0.908196 | Val Acc: 0.730813 loss: 0.842633\n",
            "saving model with acc 0.731\n",
            "[030/600] Train Acc: 0.711595 Loss: 0.904649 | Val Acc: 0.717967 loss: 0.884581\n",
            "[031/600] Train Acc: 0.712404 Loss: 0.903135 | Val Acc: 0.722764 loss: 0.858652\n",
            "[032/600] Train Acc: 0.712312 Loss: 0.899981 | Val Acc: 0.727886 loss: 0.847187\n",
            "[033/600] Train Acc: 0.713555 Loss: 0.897198 | Val Acc: 0.726829 loss: 0.843181\n",
            "[034/600] Train Acc: 0.714028 Loss: 0.894823 | Val Acc: 0.721626 loss: 0.857301\n",
            "[035/600] Train Acc: 0.714691 Loss: 0.892073 | Val Acc: 0.730081 loss: 0.836973\n",
            "[036/600] Train Acc: 0.715444 Loss: 0.890686 | Val Acc: 0.726423 loss: 0.851567\n",
            "[037/600] Train Acc: 0.715281 Loss: 0.888759 | Val Acc: 0.723008 loss: 0.857093\n",
            "[038/600] Train Acc: 0.716086 Loss: 0.886350 | Val Acc: 0.727154 loss: 0.855013\n",
            "[039/600] Train Acc: 0.716919 Loss: 0.884316 | Val Acc: 0.725203 loss: 0.846070\n",
            "[040/600] Train Acc: 0.716879 Loss: 0.882581 | Val Acc: 0.729350 loss: 0.850115\n",
            "[041/600] Train Acc: 0.717378 Loss: 0.880981 | Val Acc: 0.727236 loss: 0.844096\n",
            "[042/600] Train Acc: 0.718309 Loss: 0.878345 | Val Acc: 0.729187 loss: 0.842304\n",
            "[043/600] Train Acc: 0.717999 Loss: 0.877410 | Val Acc: 0.727236 loss: 0.838758\n",
            "[044/600] Train Acc: 0.719229 Loss: 0.874237 | Val Acc: 0.728049 loss: 0.830085\n",
            "[045/600] Train Acc: 0.719686 Loss: 0.873322 | Val Acc: 0.726098 loss: 0.838002\n",
            "[046/600] Train Acc: 0.719998 Loss: 0.871807 | Val Acc: 0.727805 loss: 0.839062\n",
            "[047/600] Train Acc: 0.719659 Loss: 0.871428 | Val Acc: 0.727073 loss: 0.845427\n",
            "[048/600] Train Acc: 0.720222 Loss: 0.869447 | Val Acc: 0.731138 loss: 0.828644\n",
            "saving model with acc 0.731\n",
            "[049/600] Train Acc: 0.720243 Loss: 0.868951 | Val Acc: 0.726667 loss: 0.839766\n",
            "[050/600] Train Acc: 0.721465 Loss: 0.866968 | Val Acc: 0.728293 loss: 0.834242\n",
            "[051/600] Train Acc: 0.721409 Loss: 0.865114 | Val Acc: 0.723415 loss: 0.841928\n",
            "[052/600] Train Acc: 0.722056 Loss: 0.864059 | Val Acc: 0.730244 loss: 0.832094\n",
            "[053/600] Train Acc: 0.722226 Loss: 0.862416 | Val Acc: 0.734146 loss: 0.826669\n",
            "saving model with acc 0.734\n",
            "[054/600] Train Acc: 0.722655 Loss: 0.861551 | Val Acc: 0.732846 loss: 0.816342\n",
            "[055/600] Train Acc: 0.722513 Loss: 0.860166 | Val Acc: 0.731626 loss: 0.820191\n",
            "[056/600] Train Acc: 0.722886 Loss: 0.859180 | Val Acc: 0.724228 loss: 0.850699\n",
            "[057/600] Train Acc: 0.723268 Loss: 0.858910 | Val Acc: 0.731626 loss: 0.827060\n",
            "[058/600] Train Acc: 0.723224 Loss: 0.856388 | Val Acc: 0.735041 loss: 0.820822\n",
            "saving model with acc 0.735\n",
            "[059/600] Train Acc: 0.724093 Loss: 0.855836 | Val Acc: 0.730325 loss: 0.830422\n",
            "[060/600] Train Acc: 0.724091 Loss: 0.855056 | Val Acc: 0.729431 loss: 0.839874\n",
            "[061/600] Train Acc: 0.723917 Loss: 0.854539 | Val Acc: 0.729024 loss: 0.829567\n",
            "[062/600] Train Acc: 0.724859 Loss: 0.852776 | Val Acc: 0.729106 loss: 0.834372\n",
            "[063/600] Train Acc: 0.725032 Loss: 0.851540 | Val Acc: 0.733252 loss: 0.817304\n",
            "[064/600] Train Acc: 0.724828 Loss: 0.851102 | Val Acc: 0.728455 loss: 0.840827\n",
            "[065/600] Train Acc: 0.725101 Loss: 0.850340 | Val Acc: 0.730894 loss: 0.830909\n",
            "[066/600] Train Acc: 0.725590 Loss: 0.849289 | Val Acc: 0.732927 loss: 0.824583\n",
            "[067/600] Train Acc: 0.726172 Loss: 0.848943 | Val Acc: 0.732927 loss: 0.824765\n",
            "[068/600] Train Acc: 0.726125 Loss: 0.847662 | Val Acc: 0.729268 loss: 0.834226\n",
            "[069/600] Train Acc: 0.726116 Loss: 0.847148 | Val Acc: 0.732846 loss: 0.821536\n",
            "[070/600] Train Acc: 0.726106 Loss: 0.846246 | Val Acc: 0.737398 loss: 0.821963\n",
            "saving model with acc 0.737\n",
            "[071/600] Train Acc: 0.726232 Loss: 0.846047 | Val Acc: 0.730000 loss: 0.835766\n",
            "[072/600] Train Acc: 0.726763 Loss: 0.844208 | Val Acc: 0.734390 loss: 0.828180\n",
            "[073/600] Train Acc: 0.726600 Loss: 0.844483 | Val Acc: 0.727967 loss: 0.828879\n",
            "[074/600] Train Acc: 0.727133 Loss: 0.842897 | Val Acc: 0.727236 loss: 0.828810\n",
            "[075/600] Train Acc: 0.726850 Loss: 0.843162 | Val Acc: 0.733740 loss: 0.816814\n",
            "[076/600] Train Acc: 0.727340 Loss: 0.842636 | Val Acc: 0.728211 loss: 0.829792\n",
            "[077/600] Train Acc: 0.727745 Loss: 0.840511 | Val Acc: 0.732114 loss: 0.822765\n",
            "[078/600] Train Acc: 0.727618 Loss: 0.840179 | Val Acc: 0.730325 loss: 0.827343\n",
            "[079/600] Train Acc: 0.727905 Loss: 0.839948 | Val Acc: 0.736829 loss: 0.819653\n",
            "[080/600] Train Acc: 0.728292 Loss: 0.839050 | Val Acc: 0.734228 loss: 0.817200\n",
            "[081/600] Train Acc: 0.727971 Loss: 0.838843 | Val Acc: 0.736098 loss: 0.824048\n",
            "[082/600] Train Acc: 0.728544 Loss: 0.837702 | Val Acc: 0.727398 loss: 0.846951\n",
            "[083/600] Train Acc: 0.728746 Loss: 0.837209 | Val Acc: 0.732358 loss: 0.823698\n",
            "[084/600] Train Acc: 0.729011 Loss: 0.836620 | Val Acc: 0.732114 loss: 0.830345\n",
            "[085/600] Train Acc: 0.728864 Loss: 0.835569 | Val Acc: 0.735041 loss: 0.817605\n",
            "[086/600] Train Acc: 0.729027 Loss: 0.835461 | Val Acc: 0.733577 loss: 0.817750\n",
            "[087/600] Train Acc: 0.729847 Loss: 0.834163 | Val Acc: 0.733008 loss: 0.825044\n",
            "[088/600] Train Acc: 0.728811 Loss: 0.834695 | Val Acc: 0.736667 loss: 0.812325\n",
            "[089/600] Train Acc: 0.729482 Loss: 0.833955 | Val Acc: 0.736911 loss: 0.809763\n",
            "[090/600] Train Acc: 0.729883 Loss: 0.832839 | Val Acc: 0.732927 loss: 0.836168\n",
            "[091/600] Train Acc: 0.729945 Loss: 0.832692 | Val Acc: 0.738618 loss: 0.812011\n",
            "saving model with acc 0.739\n",
            "[092/600] Train Acc: 0.730060 Loss: 0.831889 | Val Acc: 0.732846 loss: 0.815825\n",
            "[093/600] Train Acc: 0.730180 Loss: 0.831011 | Val Acc: 0.733089 loss: 0.818502\n",
            "[094/600] Train Acc: 0.730194 Loss: 0.830851 | Val Acc: 0.732764 loss: 0.812578\n",
            "[095/600] Train Acc: 0.730992 Loss: 0.829283 | Val Acc: 0.738211 loss: 0.804711\n",
            "[096/600] Train Acc: 0.730146 Loss: 0.830173 | Val Acc: 0.730732 loss: 0.833091\n",
            "[097/600] Train Acc: 0.730951 Loss: 0.828783 | Val Acc: 0.735610 loss: 0.817792\n",
            "[098/600] Train Acc: 0.730587 Loss: 0.828812 | Val Acc: 0.732439 loss: 0.824530\n",
            "[099/600] Train Acc: 0.731432 Loss: 0.827596 | Val Acc: 0.735122 loss: 0.806810\n",
            "[100/600] Train Acc: 0.730679 Loss: 0.828698 | Val Acc: 0.735285 loss: 0.823327\n",
            "[101/600] Train Acc: 0.731255 Loss: 0.827211 | Val Acc: 0.737561 loss: 0.810867\n",
            "[102/600] Train Acc: 0.730938 Loss: 0.827120 | Val Acc: 0.734634 loss: 0.819302\n",
            "[103/600] Train Acc: 0.731595 Loss: 0.826473 | Val Acc: 0.736098 loss: 0.816991\n",
            "[104/600] Train Acc: 0.731324 Loss: 0.826130 | Val Acc: 0.734228 loss: 0.822802\n",
            "[105/600] Train Acc: 0.731243 Loss: 0.826410 | Val Acc: 0.736911 loss: 0.815382\n",
            "[106/600] Train Acc: 0.731779 Loss: 0.825131 | Val Acc: 0.732846 loss: 0.820224\n",
            "[107/600] Train Acc: 0.731704 Loss: 0.825799 | Val Acc: 0.731545 loss: 0.829524\n",
            "[108/600] Train Acc: 0.732111 Loss: 0.824142 | Val Acc: 0.738455 loss: 0.811698\n",
            "[109/600] Train Acc: 0.732116 Loss: 0.823613 | Val Acc: 0.736423 loss: 0.819907\n",
            "[110/600] Train Acc: 0.731883 Loss: 0.823524 | Val Acc: 0.731870 loss: 0.827465\n",
            "[111/600] Train Acc: 0.732732 Loss: 0.822400 | Val Acc: 0.738130 loss: 0.812014\n",
            "[112/600] Train Acc: 0.732155 Loss: 0.823047 | Val Acc: 0.733821 loss: 0.830143\n",
            "[113/600] Train Acc: 0.732688 Loss: 0.822440 | Val Acc: 0.736992 loss: 0.810325\n",
            "[114/600] Train Acc: 0.732715 Loss: 0.821823 | Val Acc: 0.740976 loss: 0.811818\n",
            "saving model with acc 0.741\n",
            "[115/600] Train Acc: 0.732793 Loss: 0.821886 | Val Acc: 0.735366 loss: 0.819592\n",
            "[116/600] Train Acc: 0.732850 Loss: 0.821410 | Val Acc: 0.739919 loss: 0.810144\n",
            "[117/600] Train Acc: 0.732820 Loss: 0.821516 | Val Acc: 0.728943 loss: 0.834893\n",
            "[118/600] Train Acc: 0.732843 Loss: 0.820964 | Val Acc: 0.731057 loss: 0.817298\n",
            "[119/600] Train Acc: 0.733130 Loss: 0.820006 | Val Acc: 0.733333 loss: 0.807650\n",
            "[120/600] Train Acc: 0.733329 Loss: 0.819404 | Val Acc: 0.728943 loss: 0.834257\n",
            "[121/600] Train Acc: 0.733467 Loss: 0.819416 | Val Acc: 0.732602 loss: 0.823943\n",
            "[122/600] Train Acc: 0.732957 Loss: 0.820627 | Val Acc: 0.738293 loss: 0.819226\n",
            "[123/600] Train Acc: 0.733884 Loss: 0.818091 | Val Acc: 0.736423 loss: 0.812019\n",
            "[124/600] Train Acc: 0.733243 Loss: 0.818549 | Val Acc: 0.738618 loss: 0.812888\n",
            "[125/600] Train Acc: 0.733755 Loss: 0.817767 | Val Acc: 0.730081 loss: 0.826936\n",
            "[126/600] Train Acc: 0.733398 Loss: 0.817886 | Val Acc: 0.735285 loss: 0.818265\n",
            "[127/600] Train Acc: 0.733546 Loss: 0.818361 | Val Acc: 0.735528 loss: 0.805189\n",
            "[128/600] Train Acc: 0.734003 Loss: 0.817334 | Val Acc: 0.733496 loss: 0.818290\n",
            "[129/600] Train Acc: 0.734237 Loss: 0.816869 | Val Acc: 0.733008 loss: 0.815222\n",
            "[130/600] Train Acc: 0.734391 Loss: 0.816043 | Val Acc: 0.734309 loss: 0.823188\n",
            "[131/600] Train Acc: 0.733836 Loss: 0.816710 | Val Acc: 0.730976 loss: 0.829330\n",
            "[132/600] Train Acc: 0.733751 Loss: 0.816223 | Val Acc: 0.738211 loss: 0.809358\n",
            "[133/600] Train Acc: 0.734200 Loss: 0.816409 | Val Acc: 0.737073 loss: 0.819622\n",
            "[134/600] Train Acc: 0.734798 Loss: 0.814305 | Val Acc: 0.738862 loss: 0.815334\n",
            "[135/600] Train Acc: 0.734499 Loss: 0.814508 | Val Acc: 0.739756 loss: 0.811541\n",
            "[136/600] Train Acc: 0.734329 Loss: 0.815169 | Val Acc: 0.737642 loss: 0.813237\n",
            "[137/600] Train Acc: 0.734294 Loss: 0.814532 | Val Acc: 0.741870 loss: 0.800581\n",
            "saving model with acc 0.742\n",
            "[138/600] Train Acc: 0.734687 Loss: 0.815414 | Val Acc: 0.735772 loss: 0.820908\n",
            "[139/600] Train Acc: 0.734789 Loss: 0.813897 | Val Acc: 0.743008 loss: 0.798828\n",
            "saving model with acc 0.743\n",
            "[140/600] Train Acc: 0.734712 Loss: 0.814240 | Val Acc: 0.738943 loss: 0.807489\n",
            "[141/600] Train Acc: 0.735474 Loss: 0.813049 | Val Acc: 0.737317 loss: 0.809716\n",
            "[142/600] Train Acc: 0.734518 Loss: 0.814243 | Val Acc: 0.733252 loss: 0.825988\n",
            "[143/600] Train Acc: 0.734760 Loss: 0.813377 | Val Acc: 0.735122 loss: 0.812597\n",
            "[144/600] Train Acc: 0.734857 Loss: 0.813121 | Val Acc: 0.735122 loss: 0.821623\n",
            "[145/600] Train Acc: 0.734755 Loss: 0.813696 | Val Acc: 0.733089 loss: 0.821906\n",
            "[146/600] Train Acc: 0.735092 Loss: 0.811694 | Val Acc: 0.737317 loss: 0.819875\n",
            "[147/600] Train Acc: 0.734675 Loss: 0.813106 | Val Acc: 0.738699 loss: 0.804675\n",
            "[148/600] Train Acc: 0.735352 Loss: 0.811954 | Val Acc: 0.731463 loss: 0.823913\n",
            "[149/600] Train Acc: 0.735394 Loss: 0.811918 | Val Acc: 0.732195 loss: 0.823974\n",
            "[150/600] Train Acc: 0.735593 Loss: 0.811163 | Val Acc: 0.736016 loss: 0.823740\n",
            "[151/600] Train Acc: 0.736337 Loss: 0.810770 | Val Acc: 0.740244 loss: 0.807917\n",
            "[152/600] Train Acc: 0.735702 Loss: 0.810885 | Val Acc: 0.740163 loss: 0.806115\n",
            "[153/600] Train Acc: 0.735596 Loss: 0.810195 | Val Acc: 0.734228 loss: 0.827959\n",
            "[154/600] Train Acc: 0.735529 Loss: 0.810086 | Val Acc: 0.727805 loss: 0.837681\n",
            "[155/600] Train Acc: 0.735657 Loss: 0.810297 | Val Acc: 0.736748 loss: 0.815694\n",
            "[156/600] Train Acc: 0.736050 Loss: 0.809419 | Val Acc: 0.737154 loss: 0.806550\n",
            "[157/600] Train Acc: 0.736061 Loss: 0.809547 | Val Acc: 0.737236 loss: 0.805163\n",
            "[158/600] Train Acc: 0.735790 Loss: 0.809338 | Val Acc: 0.740894 loss: 0.793951\n",
            "[159/600] Train Acc: 0.735674 Loss: 0.809609 | Val Acc: 0.738455 loss: 0.804314\n",
            "[160/600] Train Acc: 0.736407 Loss: 0.808972 | Val Acc: 0.735854 loss: 0.818545\n",
            "[161/600] Train Acc: 0.735659 Loss: 0.809502 | Val Acc: 0.737073 loss: 0.816150\n",
            "[162/600] Train Acc: 0.736185 Loss: 0.808402 | Val Acc: 0.734472 loss: 0.820612\n",
            "[163/600] Train Acc: 0.736448 Loss: 0.807439 | Val Acc: 0.733008 loss: 0.816912\n",
            "[164/600] Train Acc: 0.736429 Loss: 0.808556 | Val Acc: 0.736667 loss: 0.819227\n",
            "[165/600] Train Acc: 0.736774 Loss: 0.807657 | Val Acc: 0.739024 loss: 0.807187\n",
            "[166/600] Train Acc: 0.736206 Loss: 0.807989 | Val Acc: 0.736260 loss: 0.812443\n",
            "[167/600] Train Acc: 0.736498 Loss: 0.807684 | Val Acc: 0.737886 loss: 0.814717\n",
            "[168/600] Train Acc: 0.736621 Loss: 0.807585 | Val Acc: 0.729593 loss: 0.841223\n",
            "[169/600] Train Acc: 0.736731 Loss: 0.807050 | Val Acc: 0.739675 loss: 0.808590\n",
            "[170/600] Train Acc: 0.736565 Loss: 0.806824 | Val Acc: 0.736016 loss: 0.816490\n",
            "[171/600] Train Acc: 0.736751 Loss: 0.807675 | Val Acc: 0.741789 loss: 0.806289\n",
            "[172/600] Train Acc: 0.736994 Loss: 0.805880 | Val Acc: 0.737398 loss: 0.812926\n",
            "[173/600] Train Acc: 0.736753 Loss: 0.805682 | Val Acc: 0.739593 loss: 0.810490\n",
            "[174/600] Train Acc: 0.736589 Loss: 0.806132 | Val Acc: 0.735854 loss: 0.817316\n",
            "[175/600] Train Acc: 0.737080 Loss: 0.805532 | Val Acc: 0.734797 loss: 0.822060\n",
            "[176/600] Train Acc: 0.736729 Loss: 0.806039 | Val Acc: 0.738293 loss: 0.818718\n",
            "[177/600] Train Acc: 0.736744 Loss: 0.806140 | Val Acc: 0.737561 loss: 0.815312\n",
            "[178/600] Train Acc: 0.737211 Loss: 0.804977 | Val Acc: 0.735203 loss: 0.817149\n",
            "[179/600] Train Acc: 0.737208 Loss: 0.805261 | Val Acc: 0.740976 loss: 0.802457\n",
            "[180/600] Train Acc: 0.737014 Loss: 0.805304 | Val Acc: 0.734634 loss: 0.806728\n",
            "[181/600] Train Acc: 0.737212 Loss: 0.803991 | Val Acc: 0.733902 loss: 0.818584\n",
            "[182/600] Train Acc: 0.737398 Loss: 0.804519 | Val Acc: 0.735203 loss: 0.808919\n",
            "[183/600] Train Acc: 0.737435 Loss: 0.804235 | Val Acc: 0.740894 loss: 0.801653\n",
            "[184/600] Train Acc: 0.737511 Loss: 0.803749 | Val Acc: 0.738293 loss: 0.813518\n",
            "[185/600] Train Acc: 0.737052 Loss: 0.804442 | Val Acc: 0.737236 loss: 0.810529\n",
            "[186/600] Train Acc: 0.737585 Loss: 0.803966 | Val Acc: 0.739106 loss: 0.808483\n",
            "[187/600] Train Acc: 0.737964 Loss: 0.803429 | Val Acc: 0.738374 loss: 0.802338\n",
            "[188/600] Train Acc: 0.737005 Loss: 0.804477 | Val Acc: 0.740000 loss: 0.803977\n",
            "[189/600] Train Acc: 0.737798 Loss: 0.803482 | Val Acc: 0.734797 loss: 0.816901\n",
            "[190/600] Train Acc: 0.737381 Loss: 0.803587 | Val Acc: 0.741789 loss: 0.803286\n",
            "[191/600] Train Acc: 0.737528 Loss: 0.802961 | Val Acc: 0.737967 loss: 0.809467\n",
            "[192/600] Train Acc: 0.738124 Loss: 0.801655 | Val Acc: 0.739512 loss: 0.809719\n",
            "[193/600] Train Acc: 0.737619 Loss: 0.802672 | Val Acc: 0.730244 loss: 0.822204\n",
            "[194/600] Train Acc: 0.737880 Loss: 0.803067 | Val Acc: 0.738780 loss: 0.796372\n",
            "[195/600] Train Acc: 0.737656 Loss: 0.802433 | Val Acc: 0.735935 loss: 0.809264\n",
            "[196/600] Train Acc: 0.737936 Loss: 0.802707 | Val Acc: 0.739919 loss: 0.804291\n",
            "[197/600] Train Acc: 0.738238 Loss: 0.801953 | Val Acc: 0.738618 loss: 0.818410\n",
            "[198/600] Train Acc: 0.738065 Loss: 0.802096 | Val Acc: 0.734634 loss: 0.817062\n",
            "[199/600] Train Acc: 0.737790 Loss: 0.802118 | Val Acc: 0.738293 loss: 0.819161\n",
            "[200/600] Train Acc: 0.738183 Loss: 0.800962 | Val Acc: 0.744472 loss: 0.793307\n",
            "saving model with acc 0.744\n",
            "[201/600] Train Acc: 0.738434 Loss: 0.800894 | Val Acc: 0.732683 loss: 0.820280\n",
            "[202/600] Train Acc: 0.738402 Loss: 0.800229 | Val Acc: 0.739593 loss: 0.808063\n",
            "[203/600] Train Acc: 0.737830 Loss: 0.801116 | Val Acc: 0.732276 loss: 0.823207\n",
            "[204/600] Train Acc: 0.738446 Loss: 0.800614 | Val Acc: 0.731138 loss: 0.828865\n",
            "[205/600] Train Acc: 0.738111 Loss: 0.800426 | Val Acc: 0.736748 loss: 0.816699\n",
            "[206/600] Train Acc: 0.738433 Loss: 0.800200 | Val Acc: 0.735935 loss: 0.810065\n",
            "[207/600] Train Acc: 0.738211 Loss: 0.800540 | Val Acc: 0.736423 loss: 0.814519\n",
            "[208/600] Train Acc: 0.738531 Loss: 0.800665 | Val Acc: 0.734228 loss: 0.816831\n",
            "[209/600] Train Acc: 0.737852 Loss: 0.801870 | Val Acc: 0.729431 loss: 0.829368\n",
            "[210/600] Train Acc: 0.738593 Loss: 0.799782 | Val Acc: 0.744472 loss: 0.793950\n",
            "[211/600] Train Acc: 0.738811 Loss: 0.799654 | Val Acc: 0.733821 loss: 0.819152\n",
            "[212/600] Train Acc: 0.739053 Loss: 0.798669 | Val Acc: 0.730325 loss: 0.820505\n",
            "[213/600] Train Acc: 0.738680 Loss: 0.799732 | Val Acc: 0.737073 loss: 0.809316\n",
            "[214/600] Train Acc: 0.738760 Loss: 0.799223 | Val Acc: 0.737317 loss: 0.801345\n",
            "[215/600] Train Acc: 0.738618 Loss: 0.799434 | Val Acc: 0.742846 loss: 0.796930\n",
            "[216/600] Train Acc: 0.738990 Loss: 0.799320 | Val Acc: 0.739919 loss: 0.796948\n",
            "[217/600] Train Acc: 0.738969 Loss: 0.799302 | Val Acc: 0.744228 loss: 0.795736\n",
            "[218/600] Train Acc: 0.739140 Loss: 0.798017 | Val Acc: 0.731626 loss: 0.820392\n",
            "[219/600] Train Acc: 0.739078 Loss: 0.798597 | Val Acc: 0.736179 loss: 0.811356\n",
            "[220/600] Train Acc: 0.738932 Loss: 0.798456 | Val Acc: 0.731138 loss: 0.817552\n",
            "[221/600] Train Acc: 0.738672 Loss: 0.799338 | Val Acc: 0.742683 loss: 0.803405\n",
            "[222/600] Train Acc: 0.739248 Loss: 0.797550 | Val Acc: 0.741463 loss: 0.801706\n",
            "[223/600] Train Acc: 0.739204 Loss: 0.797805 | Val Acc: 0.742114 loss: 0.804799\n",
            "[224/600] Train Acc: 0.739325 Loss: 0.797809 | Val Acc: 0.740569 loss: 0.801616\n",
            "[225/600] Train Acc: 0.739017 Loss: 0.798006 | Val Acc: 0.734797 loss: 0.820130\n",
            "[226/600] Train Acc: 0.738647 Loss: 0.798432 | Val Acc: 0.739675 loss: 0.805574\n",
            "[227/600] Train Acc: 0.739901 Loss: 0.796363 | Val Acc: 0.738049 loss: 0.809721\n",
            "[228/600] Train Acc: 0.739461 Loss: 0.797632 | Val Acc: 0.739837 loss: 0.809082\n",
            "[229/600] Train Acc: 0.739590 Loss: 0.797208 | Val Acc: 0.739756 loss: 0.806796\n",
            "[230/600] Train Acc: 0.739896 Loss: 0.796405 | Val Acc: 0.735122 loss: 0.826160\n",
            "[231/600] Train Acc: 0.739714 Loss: 0.796536 | Val Acc: 0.739919 loss: 0.808372\n",
            "[232/600] Train Acc: 0.739389 Loss: 0.796989 | Val Acc: 0.739106 loss: 0.815776\n",
            "[233/600] Train Acc: 0.739561 Loss: 0.797361 | Val Acc: 0.739593 loss: 0.809658\n",
            "[234/600] Train Acc: 0.739798 Loss: 0.796787 | Val Acc: 0.737154 loss: 0.815014\n",
            "[235/600] Train Acc: 0.739515 Loss: 0.795547 | Val Acc: 0.733740 loss: 0.813533\n",
            "[236/600] Train Acc: 0.739278 Loss: 0.796311 | Val Acc: 0.739350 loss: 0.808153\n",
            "[237/600] Train Acc: 0.739530 Loss: 0.796337 | Val Acc: 0.737073 loss: 0.823598\n",
            "[238/600] Train Acc: 0.739999 Loss: 0.795450 | Val Acc: 0.742195 loss: 0.808623\n",
            "[239/600] Train Acc: 0.740221 Loss: 0.795144 | Val Acc: 0.742276 loss: 0.803897\n",
            "[240/600] Train Acc: 0.739754 Loss: 0.796758 | Val Acc: 0.737886 loss: 0.815420\n",
            "[241/600] Train Acc: 0.740041 Loss: 0.795072 | Val Acc: 0.742276 loss: 0.806860\n",
            "[242/600] Train Acc: 0.739866 Loss: 0.795264 | Val Acc: 0.735528 loss: 0.814619\n",
            "[243/600] Train Acc: 0.740291 Loss: 0.794401 | Val Acc: 0.737398 loss: 0.811322\n",
            "[244/600] Train Acc: 0.740241 Loss: 0.794404 | Val Acc: 0.745285 loss: 0.796424\n",
            "saving model with acc 0.745\n",
            "[245/600] Train Acc: 0.739655 Loss: 0.795733 | Val Acc: 0.733659 loss: 0.827008\n",
            "[246/600] Train Acc: 0.739801 Loss: 0.795189 | Val Acc: 0.742358 loss: 0.801139\n",
            "[247/600] Train Acc: 0.740254 Loss: 0.795918 | Val Acc: 0.745935 loss: 0.795348\n",
            "saving model with acc 0.746\n",
            "[248/600] Train Acc: 0.740152 Loss: 0.794694 | Val Acc: 0.743171 loss: 0.795752\n",
            "[249/600] Train Acc: 0.740166 Loss: 0.794798 | Val Acc: 0.737642 loss: 0.808403\n",
            "[250/600] Train Acc: 0.739766 Loss: 0.795447 | Val Acc: 0.739512 loss: 0.794621\n",
            "[251/600] Train Acc: 0.739981 Loss: 0.794248 | Val Acc: 0.738455 loss: 0.811022\n",
            "[252/600] Train Acc: 0.739819 Loss: 0.794937 | Val Acc: 0.738537 loss: 0.809074\n",
            "[253/600] Train Acc: 0.739523 Loss: 0.795393 | Val Acc: 0.741870 loss: 0.800574\n",
            "[254/600] Train Acc: 0.740877 Loss: 0.793327 | Val Acc: 0.735122 loss: 0.818475\n",
            "[255/600] Train Acc: 0.740073 Loss: 0.793377 | Val Acc: 0.734146 loss: 0.820921\n",
            "[256/600] Train Acc: 0.740089 Loss: 0.794069 | Val Acc: 0.736911 loss: 0.815445\n",
            "[257/600] Train Acc: 0.740635 Loss: 0.793812 | Val Acc: 0.743333 loss: 0.799841\n",
            "[258/600] Train Acc: 0.740555 Loss: 0.793803 | Val Acc: 0.740732 loss: 0.800336\n",
            "[259/600] Train Acc: 0.740642 Loss: 0.793976 | Val Acc: 0.737642 loss: 0.812721\n",
            "[260/600] Train Acc: 0.740831 Loss: 0.792952 | Val Acc: 0.738374 loss: 0.803490\n",
            "[261/600] Train Acc: 0.740034 Loss: 0.793987 | Val Acc: 0.735122 loss: 0.809714\n",
            "[262/600] Train Acc: 0.739933 Loss: 0.793742 | Val Acc: 0.739512 loss: 0.805067\n",
            "[263/600] Train Acc: 0.739925 Loss: 0.793215 | Val Acc: 0.735691 loss: 0.812578\n",
            "[264/600] Train Acc: 0.741011 Loss: 0.792710 | Val Acc: 0.735610 loss: 0.819713\n",
            "[265/600] Train Acc: 0.740811 Loss: 0.792301 | Val Acc: 0.740569 loss: 0.805726\n",
            "[266/600] Train Acc: 0.740516 Loss: 0.792563 | Val Acc: 0.738211 loss: 0.811741\n",
            "[267/600] Train Acc: 0.740004 Loss: 0.794012 | Val Acc: 0.741057 loss: 0.803389\n",
            "[268/600] Train Acc: 0.740376 Loss: 0.792495 | Val Acc: 0.737724 loss: 0.806407\n",
            "[269/600] Train Acc: 0.740847 Loss: 0.791542 | Val Acc: 0.735691 loss: 0.819767\n",
            "[270/600] Train Acc: 0.741102 Loss: 0.791223 | Val Acc: 0.738374 loss: 0.810123\n",
            "[271/600] Train Acc: 0.740709 Loss: 0.792772 | Val Acc: 0.738699 loss: 0.808729\n",
            "[272/600] Train Acc: 0.741316 Loss: 0.791512 | Val Acc: 0.735447 loss: 0.814971\n",
            "[273/600] Train Acc: 0.740609 Loss: 0.792593 | Val Acc: 0.741463 loss: 0.796815\n",
            "[274/600] Train Acc: 0.740870 Loss: 0.792280 | Val Acc: 0.736504 loss: 0.815049\n",
            "[275/600] Train Acc: 0.740878 Loss: 0.792570 | Val Acc: 0.741301 loss: 0.806254\n",
            "[276/600] Train Acc: 0.740937 Loss: 0.791379 | Val Acc: 0.742033 loss: 0.793902\n",
            "[277/600] Train Acc: 0.740234 Loss: 0.792829 | Val Acc: 0.736016 loss: 0.816742\n",
            "[278/600] Train Acc: 0.741314 Loss: 0.790823 | Val Acc: 0.739431 loss: 0.806009\n",
            "[279/600] Train Acc: 0.740922 Loss: 0.790850 | Val Acc: 0.735854 loss: 0.816666\n",
            "[280/600] Train Acc: 0.741076 Loss: 0.790656 | Val Acc: 0.744878 loss: 0.790706\n",
            "[281/600] Train Acc: 0.740160 Loss: 0.792244 | Val Acc: 0.739675 loss: 0.810340\n",
            "[282/600] Train Acc: 0.741181 Loss: 0.790341 | Val Acc: 0.730244 loss: 0.827705\n",
            "[283/600] Train Acc: 0.740762 Loss: 0.790635 | Val Acc: 0.740813 loss: 0.807728\n",
            "[284/600] Train Acc: 0.741186 Loss: 0.790085 | Val Acc: 0.740894 loss: 0.802794\n",
            "[285/600] Train Acc: 0.741469 Loss: 0.789552 | Val Acc: 0.733902 loss: 0.812920\n",
            "[286/600] Train Acc: 0.741329 Loss: 0.790339 | Val Acc: 0.735610 loss: 0.806134\n",
            "[287/600] Train Acc: 0.741076 Loss: 0.790763 | Val Acc: 0.730894 loss: 0.833806\n",
            "[288/600] Train Acc: 0.741658 Loss: 0.789224 | Val Acc: 0.743902 loss: 0.792101\n",
            "[289/600] Train Acc: 0.741477 Loss: 0.789840 | Val Acc: 0.741870 loss: 0.806798\n",
            "[290/600] Train Acc: 0.741457 Loss: 0.790016 | Val Acc: 0.740894 loss: 0.809931\n",
            "[291/600] Train Acc: 0.741303 Loss: 0.790212 | Val Acc: 0.741870 loss: 0.804102\n",
            "[292/600] Train Acc: 0.741825 Loss: 0.790236 | Val Acc: 0.736098 loss: 0.820799\n",
            "[293/600] Train Acc: 0.741269 Loss: 0.790005 | Val Acc: 0.736179 loss: 0.814556\n",
            "[294/600] Train Acc: 0.741208 Loss: 0.790103 | Val Acc: 0.738618 loss: 0.812946\n",
            "[295/600] Train Acc: 0.741615 Loss: 0.788738 | Val Acc: 0.741301 loss: 0.799776\n",
            "[296/600] Train Acc: 0.741101 Loss: 0.790292 | Val Acc: 0.746585 loss: 0.801021\n",
            "saving model with acc 0.747\n",
            "[297/600] Train Acc: 0.741742 Loss: 0.789329 | Val Acc: 0.738374 loss: 0.812157\n",
            "[298/600] Train Acc: 0.741603 Loss: 0.790268 | Val Acc: 0.741138 loss: 0.805330\n",
            "[299/600] Train Acc: 0.741627 Loss: 0.788820 | Val Acc: 0.745854 loss: 0.788621\n",
            "[300/600] Train Acc: 0.742071 Loss: 0.788799 | Val Acc: 0.740081 loss: 0.806297\n",
            "[301/600] Train Acc: 0.741939 Loss: 0.788645 | Val Acc: 0.737886 loss: 0.808504\n",
            "[302/600] Train Acc: 0.741525 Loss: 0.789352 | Val Acc: 0.740813 loss: 0.809944\n",
            "[303/600] Train Acc: 0.741858 Loss: 0.788699 | Val Acc: 0.737480 loss: 0.801720\n",
            "[304/600] Train Acc: 0.741502 Loss: 0.789554 | Val Acc: 0.739350 loss: 0.810699\n",
            "[305/600] Train Acc: 0.741636 Loss: 0.789621 | Val Acc: 0.739593 loss: 0.804492\n",
            "[306/600] Train Acc: 0.741473 Loss: 0.788989 | Val Acc: 0.744390 loss: 0.801596\n",
            "[307/600] Train Acc: 0.741686 Loss: 0.789239 | Val Acc: 0.732602 loss: 0.822463\n",
            "[308/600] Train Acc: 0.741676 Loss: 0.789196 | Val Acc: 0.736016 loss: 0.803376\n",
            "[309/600] Train Acc: 0.741894 Loss: 0.788624 | Val Acc: 0.738780 loss: 0.818818\n",
            "[310/600] Train Acc: 0.742393 Loss: 0.787135 | Val Acc: 0.732033 loss: 0.823984\n",
            "[311/600] Train Acc: 0.741129 Loss: 0.789406 | Val Acc: 0.738293 loss: 0.807851\n",
            "[312/600] Train Acc: 0.741675 Loss: 0.788944 | Val Acc: 0.732602 loss: 0.827721\n",
            "[313/600] Train Acc: 0.742021 Loss: 0.788444 | Val Acc: 0.731138 loss: 0.832274\n",
            "[314/600] Train Acc: 0.742012 Loss: 0.788315 | Val Acc: 0.737967 loss: 0.812493\n",
            "[315/600] Train Acc: 0.742292 Loss: 0.787451 | Val Acc: 0.738455 loss: 0.805991\n",
            "[316/600] Train Acc: 0.741815 Loss: 0.787573 | Val Acc: 0.740244 loss: 0.810454\n",
            "[317/600] Train Acc: 0.742004 Loss: 0.788083 | Val Acc: 0.740488 loss: 0.804159\n",
            "[318/600] Train Acc: 0.741798 Loss: 0.788321 | Val Acc: 0.744553 loss: 0.798541\n",
            "[319/600] Train Acc: 0.742062 Loss: 0.788005 | Val Acc: 0.744959 loss: 0.797747\n",
            "[320/600] Train Acc: 0.742210 Loss: 0.788256 | Val Acc: 0.742276 loss: 0.798459\n",
            "[321/600] Train Acc: 0.741948 Loss: 0.787815 | Val Acc: 0.736341 loss: 0.808207\n",
            "[322/600] Train Acc: 0.742302 Loss: 0.787143 | Val Acc: 0.735935 loss: 0.816895\n",
            "[323/600] Train Acc: 0.742320 Loss: 0.787168 | Val Acc: 0.736504 loss: 0.811263\n",
            "[324/600] Train Acc: 0.742312 Loss: 0.787303 | Val Acc: 0.740081 loss: 0.799705\n",
            "[325/600] Train Acc: 0.742517 Loss: 0.787142 | Val Acc: 0.740488 loss: 0.802917\n",
            "[326/600] Train Acc: 0.742545 Loss: 0.786639 | Val Acc: 0.742683 loss: 0.787009\n",
            "[327/600] Train Acc: 0.742212 Loss: 0.787039 | Val Acc: 0.734797 loss: 0.816506\n",
            "[328/600] Train Acc: 0.742288 Loss: 0.787372 | Val Acc: 0.738211 loss: 0.806304\n",
            "[329/600] Train Acc: 0.742459 Loss: 0.786968 | Val Acc: 0.736341 loss: 0.814593\n",
            "[330/600] Train Acc: 0.742537 Loss: 0.787571 | Val Acc: 0.734634 loss: 0.816379\n",
            "[331/600] Train Acc: 0.742407 Loss: 0.786457 | Val Acc: 0.738374 loss: 0.803057\n",
            "[332/600] Train Acc: 0.741718 Loss: 0.787664 | Val Acc: 0.737886 loss: 0.801576\n",
            "[333/600] Train Acc: 0.742737 Loss: 0.785456 | Val Acc: 0.739268 loss: 0.799308\n",
            "[334/600] Train Acc: 0.742257 Loss: 0.786815 | Val Acc: 0.737967 loss: 0.808565\n",
            "[335/600] Train Acc: 0.742669 Loss: 0.785696 | Val Acc: 0.739106 loss: 0.802873\n",
            "[336/600] Train Acc: 0.742499 Loss: 0.786436 | Val Acc: 0.740000 loss: 0.812855\n",
            "[337/600] Train Acc: 0.742499 Loss: 0.786019 | Val Acc: 0.739024 loss: 0.809658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PKjtAScPWtr"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}